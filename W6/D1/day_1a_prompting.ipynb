{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9782fd1",
   "metadata": {},
   "source": [
    "# Day 1a - Prompting with OpenRouter AI\n",
    "\n",
    "This tutorial introduces you to the fundamentals of working with OpenRouter AI, including prompt engineering techniques and code generation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Use the Gemini API to generate content\n",
    "- Apply various prompt engineering techniques (zero-shot, few-shot, chain-of-thought)\n",
    "- Control generation parameters (temperature, top-p, max_output_tokens)\n",
    "- Generate structured outputs using schemas\n",
    "- Generate, execute, and explain code\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "- Obtained an OpenRouter API key from [OpenRouter](https://openrouter.ai/keys)\n",
    "- Installed the required dependencies listed in `pyproject.toml` via `uv sync`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8331a4",
   "metadata": {},
   "source": [
    "## Part 1: Getting Started with OpenRouter AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59927e3",
   "metadata": {},
   "source": [
    "### Import the SDK and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70144ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f456a",
   "metadata": {},
   "source": [
    "### Initialize the Client\n",
    "\n",
    "OpenRouter provides a unified API that gives you access to hundreds of AI models through a single endpoint.\n",
    "We use the OpenAI SDK with OpenRouter's base URL for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5195cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from environment variable or use the commented line for Colab\n",
    "# For Colab: import google.colab.userdata; api_key = google.colab.userdata.get('OPENROUTER_API_KEY')\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set OPENROUTER_API_KEY environment variable. Get your key from https://openrouter.ai/keys\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0dcfe6",
   "metadata": {},
   "source": [
    "### Choose a Model\n",
    "\n",
    "OpenRouter provides access to hundreds of AI models from various providers.\n",
    "Each model has different capabilities, token limits, and performance characteristics.\n",
    "You can list all available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ed608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models from OpenRouter\n",
    "import requests\n",
    "\n",
    "models_response = requests.get(\n",
    "    \"https://openrouter.ai/api/v1/models\",\n",
    "    headers={\"Authorization\": f\"Bearer {api_key}\"}\n",
    ")\n",
    "models_data = models_response.json()\n",
    "\n",
    "# Print model IDs\n",
    "for model in models_data.get(\"data\", [])[:20]:  # Show first 20 models\n",
    "    print(f\"{model.get('id')} - {model.get('name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model to use\n",
    "# You can use any model available on OpenRouter.\n",
    "# Check https://openrouter.ai/models for available models.\n",
    "DEFAULT_MODEL = \"deepseek/deepseek-chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1752f5",
   "metadata": {},
   "source": [
    "### Run Your First Prompt\n",
    "\n",
    "Let's start with a simple text generation request. We'll use a fast and efficient model suitable for most tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=100,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is OpenRouter AI?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f831f5",
   "metadata": {},
   "source": [
    "The response often comes back in markdown format, which you can render directly in notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fa8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e0fbf",
   "metadata": {},
   "source": [
    "### Start a Chat\n",
    "\n",
    "The previous example uses a single-turn, text-in/text-out structure. You can also set up a multi-turn chat where the conversation state persists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25697a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For chat, we maintain conversation history manually\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"Hello! My name is Adam. And I am 11 years old.\"})\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=200,\n",
    "    messages=messages\n",
    ")\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"Are you happy?\"})\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=200,\n",
    "    messages=messages\n",
    ")\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a303e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The messages list maintains conversation state\n",
    "messages.append({\"role\": \"user\", \"content\": \"Who am I?\"})\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=200,\n",
    "    messages=messages\n",
    ")\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e9cbb",
   "metadata": {},
   "source": [
    "## Part 2: Generation Parameters\n",
    "\n",
    "Generation parameters allow you to control how the model generates text. Understanding these parameters is crucial for getting the best results for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4d0dc",
   "metadata": {},
   "source": [
    "### Output Length\n",
    "\n",
    "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_tokens` parameter. This parameter stops generation once the specified length is reached, but it doesn't influence the style or content of the output. You may need to adjust your prompt to get a complete response within the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a 1000 word essay on the importance of olives in modern society.\"}\n",
    "    ],\n",
    "    max_tokens=25\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nLength: {len(response.choices[0].message.content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a more appropriate prompt for the token limit\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem on the importance of olives in modern society.\"}\n",
    "    ],\n",
    "    max_tokens=25\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d8d87",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in more diverse and creative outputs, while lower temperatures produce more deterministic and focused results.\n",
    "\n",
    "- **High temperature (1.0-2.0)**: More creative, diverse outputs\n",
    "- **Low temperature (0.0-0.5)**: More deterministic, focused outputs\n",
    "- **Temperature 0.0**: Greedy decoding (selects the most probable token at each step)\n",
    "\n",
    "Temperature doesn't provide guarantees of randomness, but it can be used to \"nudge\" the output in the desired direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Pick a random colour... (respond in a single word)\"}\n",
    "        ],\n",
    "        temperature=2.0\n",
    "    )\n",
    "    \n",
    "    if response.choices[0].message.content:\n",
    "        print(response.choices[0].message.content, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with low temperature\n",
    "for _ in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Pick a random colour... (respond in a single word)\"}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    if response.choices[0].message.content:\n",
    "        print(response.choices[0].message.content, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6bf80",
   "metadata": {},
   "source": [
    "### Top-P\n",
    "\n",
    "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates.\n",
    "A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects from every token in the model's vocabulary.\n",
    "\n",
    "**Note:** Top-K is not configurable in the Gemini 2.5 series of models, but can be changed in older models.\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be918ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": story_prompt}\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584d052",
   "metadata": {},
   "source": [
    "## Part 3: Prompt Engineering Techniques\n",
    "\n",
    "Prompt engineering is the practice of designing effective prompts to get the best results from language models.\n",
    "This section covers several key techniques based on the [Gemini API prompting strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c50a5",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompts describe the request directly without providing examples.\n",
    "The model relies on its training to understand and complete the task.\n",
    "\n",
    "Zero-shot prompting works well for:\n",
    "- Simple classification tasks\n",
    "- Well-defined tasks the model was trained on\n",
    "- When you want to avoid providing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = (\n",
    "    \"Classify restaurant reviews as POSITIVE, NEUTRAL or NEGATIVE.\"\n",
    "    \"\\nReview: 'The food was great and the service was excellent. Delightful food and service.'\",\n",
    "    \"\\nSentiment: \"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": zero_shot_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_tokens=5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410e0df",
   "metadata": {},
   "source": [
    "#### Enum Mode\n",
    "\n",
    "Sometimes models can produce more text than you want, or include explanatory text. The Gemini API has an **Enum mode** feature that allows you to constrain the output to a fixed set of values. This ensures you get exactly one of the specified options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ff376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "# For structured output, we use JSON mode with a schema description\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"{zero_shot_prompt}\n",
    "Respond with only one word: POSITIVE, NEUTRAL, or NEGATIVE.\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8abf5",
   "metadata": {},
   "source": [
    "When using constrained output like an enum, the Python SDK will attempt to convert the model's text response into a Python object automatically. It's stored in the `response.parsed` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response to get the enum value\n",
    "response_text = response.choices[0].message.content.strip().upper()\n",
    "try:\n",
    "    enum_response = Sentiment[response_text]\n",
    "except (KeyError, ValueError):\n",
    "    # Fallback: try to match the value\n",
    "    for sentiment in Sentiment:\n",
    "        if sentiment.value.upper() in response_text or response_text in sentiment.value.upper():\n",
    "            enum_response = sentiment\n",
    "            break\n",
    "    else:\n",
    "        enum_response = None\n",
    "\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9bcb63",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting\n",
    "\n",
    "Providing examples of the expected response is known as \"few-shot\" prompting. When you provide one example, it's \"one-shot\"; multiple examples make it \"few-shot.\"\n",
    "\n",
    "Few-shot prompting works well for:\n",
    "- Tasks with specific output formats\n",
    "- When you want to demonstrate the desired style or structure\n",
    "- Complex tasks that benefit from examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883dfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = (\n",
    "    \"apple -> a.p.p.l.e\"\n",
    "    \"\\nbanana -> b.a.n.a.n.a\"\n",
    "    \"\\ncherry -> c.h.e.r.r.y\"\n",
    ")\n",
    "\n",
    "user_input = \"berry -> \"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt + \"\\n\" + user_input}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375bbb2",
   "metadata": {},
   "source": [
    "#### JSON Mode\n",
    "\n",
    "To ensure you only receive JSON (with no other text or markdown), and to provide control over the schema, you can use the Gemini API's **JSON mode**. This forces the model to constrain decoding according to the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b49a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "# Use JSON mode for structured output\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=250,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Parse this pizza order into JSON with fields: size, ingredients (array), and type.\n",
    "Order: Can I have a large dessert pizza with apple and chocolate\n",
    "Respond with only valid JSON, no other text.\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24304fba",
   "metadata": {},
   "source": [
    "### Chain of Thought (CoT) Prompting\n",
    "\n",
    "Direct prompting can return answers quickly, but they can be prone to errors, especially for reasoning tasks. Chain-of-Thought prompting instructs the model to output intermediate reasoning steps, which typically leads to better results, especially when combined with few-shot examples.\n",
    "\n",
    "**Note:** This technique doesn't completely eliminate errors, and it tends to cost more due to increased token usage. However, it's very effective for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68242f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=250,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06092f",
   "metadata": {},
   "source": [
    "Now try the same problem with chain-of-thought prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"When I was 4 years old, my partner was 3 times my age. Now,\"\n",
    "    \"I am 20 years old. How old is my partner?\"\n",
    ")\n",
    "\n",
    "cot_trigger = \"Let's think step by step.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=250,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt + \" \" + cot_trigger}\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085060a1",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "\n",
    "System instructions allow you to set the behavior, tone, and role of the model for the entire conversation. This is more efficient than including instructions in every user message.\n",
    "\n",
    "System instructions are useful for:\n",
    "- Setting the model's role (e.g., \"You are a helpful assistant\")\n",
    "- Defining output format preferences\n",
    "- Establishing guidelines that apply to all interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d02a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a helpful coding assistant.\"\n",
    "    \" Always provide code examples with clear explanations.\"\n",
    "    \" Use Python 3.10+ syntax.\"\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=250,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do I read a CSV file in Python?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7bebe",
   "metadata": {},
   "source": [
    "### Thinking Mode\n",
    "\n",
    "Some models on OpenRouter support \"thinking\" or reasoning modes that generate intermediate reasoning steps.\n",
    "These models can provide high-quality responses without needing specialized prompting techniques.\n",
    "\n",
    "**Note:** Check [OpenRouter Models](https://openrouter.ai/models) for models with reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Use streaming for real-time response\n",
    "stream = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who was the youngest author listed on the transformers NLP paper?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        buf.write(content)\n",
    "        # Display the response as it is streamed\n",
    "        print(content, end='', flush=True)\n",
    "\n",
    "# And then render the finished response as formatted markdown\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd550ee",
   "metadata": {},
   "source": [
    "## Part 4: Code Generation\n",
    "\n",
    "The Gemini family of models can generate code, configuration files, and scripts. This is helpful when learning to code, learning a new language, or rapidly generating a first draft.\n",
    "\n",
    "**Important:** Since LLMs can make mistakes and may repeat training data, it's essential to read and test your code first, and comply with any relevant licenses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97544b7b",
   "metadata": {},
   "source": [
    "### Generating Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = (\n",
    "    \"Write a Python function to calculate the factorial of a number.\"\n",
    "    \" No explanation, provide only the code.\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": code_prompt}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e740566",
   "metadata": {},
   "source": [
    "### Explaining Code\n",
    "\n",
    "The Gemini models can also explain code to you. This is useful for understanding unfamiliar codebases or learning new programming concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explain a simple Python function\n",
    "code_to_explain = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this function does, how it works, and what its time complexity is.\n",
    "\n",
    "```python\n",
    "{code_to_explain}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": explain_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d629353",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you've learned:\n",
    "\n",
    "1. **Getting Started**: How to initialize the OpenRouter API client and make basic requests\n",
    "2. **Generation Parameters**: How to control output length, temperature, and top-p\n",
    "3. **Prompt Engineering**: Zero-shot, few-shot, chain-of-thought, and system instructions\n",
    "4. **Structured Output**: Using enums and JSON schemas to constrain model outputs\n",
    "5. **Code Generation**: Generating and explaining code"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
