{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic 100k Reviews (Part 2): Classifier\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is the second part of the Arabic 100k Reviews classification pipeline. Building on the preprocessing work from Part 1, we now focus on building and evaluating a text classification model to predict sentiment (Positive or Negative) from Arabic reviews. This notebook covers vectorization, model training, evaluation, and interpretation of results, demonstrating a complete end-to-end NLP classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.6/332.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for codernitydb3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install farasapy==0.1.1 ir-datasets==0.5.11 ir-measures==0.4.3 joblib==1.5.3 kaggle==1.8.3 matplotlib==3.10.8 nltk==3.9.2 numpy==1.26.4 pandas==2.3.3 pyarabic==0.6.15 pyspellchecker==0.8.4 qalsadi==0.5.1 scikit-learn==1.8.0 seaborn==0.13.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Build a text classification model for Arabic sentiment analysis\n",
    "- Apply vectorization techniques (TF-IDF) to convert preprocessed text into numerical features\n",
    "- Train and evaluate a machine learning classifier (Logistic Regression)\n",
    "- Understand model evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix)\n",
    "- Interpret classification results and analyze model performance\n",
    "- Recognize the importance of proper train/test splitting and data preprocessing\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Setup and Imports** - Installing dependencies and importing libraries\n",
    "2. **Data Loading** - Loading preprocessed data from Part 1\n",
    "3. **Text Analytics (EDA)** - Analyzing the preprocessed dataset\n",
    "4. **Vectorization** - Converting text to numerical features using TF-IDF\n",
    "5. **Model Training** - Training a Logistic Regression classifier\n",
    "6. **Model Evaluation** - Assessing performance with various metrics\n",
    "7. **Results Interpretation** - Understanding model predictions and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'sample_data']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1957766846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ar_reviews_100k_cleaned.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_normal = pd.read_csv('ar_reviews_100k_cleaned.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = df_normal.loc[45:50].index\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.loc[sample_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this, let's count the number of unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = [item for sublist in df_normal['stemmed_tokens'] for item in sublist]\n",
    "token_counter = Counter(all_tokens)\n",
    "token_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique words do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ratio = len(token_counter) / len(all_tokens)\n",
    "print(f'Unique ratio: {unique_ratio:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out which words are associated with positive and negative labels, and which aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split counts based on labels\n",
    "counter_positive = Counter([item for sublist in df_normal.loc[df_normal['label'] == 'Positive', 'stemmed_tokens'] for item in sublist])\n",
    "counter_negative = Counter([item for sublist in df_normal.loc[df_normal['label'] == 'Negative', 'stemmed_tokens'] for item in sublist])\n",
    "counter_mixed = Counter([item for sublist in df_normal.loc[df_normal['label'] == 'Mixed', 'stemmed_tokens'] for item in sublist])\n",
    "\n",
    "# Purify each label\n",
    "## Positive\n",
    "pure_positive = counter_positive.copy()\n",
    "pure_positive.subtract(counter_negative)\n",
    "pure_positive.subtract(counter_mixed)\n",
    "\n",
    "## Negative\n",
    "pure_negative = counter_negative.copy()\n",
    "pure_negative.subtract(counter_positive)\n",
    "pure_negative.subtract(counter_mixed)\n",
    "\n",
    "## Neutral\n",
    "pure_mixed = counter_mixed.copy()\n",
    "pure_mixed.subtract(counter_positive)\n",
    "pure_mixed.subtract(counter_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    pure_positive.most_common(20),\n",
    "    columns=['token', 'count']\n",
    ").style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    pure_negative.most_common(20),\n",
    "    columns=['token', 'count']\n",
    ").style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the neutral words that are not in the positive or negative\n",
    "pd.DataFrame(\n",
    "    pure_mixed.most_common(20),\n",
    "    columns=['token', 'count']\n",
    ").style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.loc[sample_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Text Classification\n",
    "\n",
    "Now that we have cleaned and preprocessed our text data, we can build a **text classifier** that predicts the sentiment (Positive, Negative, or Mixed) of Arabic reviews.\n",
    "\n",
    "**What is Text Classification?**\n",
    "\n",
    "Text classification is a supervised machine learning task where we:\n",
    "1. **Extract features** from text (convert text to numbers)\n",
    "2. **Train a model** to learn patterns between features and labels\n",
    "3. **Predict** the class of new, unseen text\n",
    "\n",
    "**Why use word counts?**\n",
    "\n",
    "After cleaning and preprocessing, we have a list of tokens (words) for each review. One simple but effective approach is to:\n",
    "- Count how many times each word appears in each document\n",
    "- Use these counts as features for our classifier\n",
    "- This is called **Bag of Words (BoW)** representation\n",
    "\n",
    "**The Bag of Words Model:**\n",
    "\n",
    "The Bag of Words model represents text as a vector of word counts, ignoring word order and grammar. For example:\n",
    "\n",
    "- Review 1: \"ممتاز رائع\" → `{\"ممتاز\": 1, \"رائع\": 1}`\n",
    "- Review 2: \"ممتاز ممتاز سيء\" → `{\"ممتاز\": 2, \"رائع\": 0, \"سيء\": 1}`\n",
    "\n",
    "This creates a feature matrix where:\n",
    "- Each row = one review\n",
    "- Each column = one unique word in the vocabulary\n",
    "- Each cell = count of that word in that review\n",
    "\n",
    "**Why this works:**\n",
    "\n",
    "From our EDA, we saw that certain words are more associated with positive reviews (e.g., \"ممتاز\", \"رائع\") and others with negative reviews (e.g., \"سيء\", \"ضعيف\"). A classifier can learn these patterns from the word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Text Data for Feature Extraction\n",
    "\n",
    "Our `stemmed_tokens` column contains lists of tokens. To use scikit-learn's `CountVectorizer`, we need to convert these token lists back into text strings (space-separated words).\n",
    "\n",
    "**Why convert back to strings?**\n",
    "\n",
    "- `CountVectorizer` expects text input (strings)\n",
    "- It will handle tokenization internally, but since we've already cleaned and stemmed our text, we want to use our preprocessed tokens\n",
    "- We'll join the tokens with spaces to create clean text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token lists back to space-separated strings\n",
    "# This allows CountVectorizer to work with our preprocessed tokens\n",
    "df_normal['text_processed'] = df_normal['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Original text:\")\n",
    "print(df_normal.loc[sample_ids[0], 'text'])\n",
    "print(\"\\nCleaned and processed text:\")\n",
    "print(df_normal.loc[sample_ids[0], 'text_processed'])\n",
    "print(\"\\nTokens used:\")\n",
    "print(df_normal.loc[sample_ids[0], 'stemmed_tokens'][:10])  # Show first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Word Count Features\n",
    "\n",
    "We'll use scikit-learn's `CountVectorizer` to convert our processed text into a matrix of word counts.\n",
    "\n",
    "**Key parameters:**\n",
    "- `max_features`: Limit vocabulary size to the most frequent N words (reduces dimensionality)\n",
    "- `min_df`: Ignore words that appear in fewer than N documents (removes rare words)\n",
    "- `max_df`: Ignore words that appear in more than N% of documents (removes common words that appear everywhere)\n",
    "\n",
    "**Why limit features?**\n",
    "- Reduces memory usage\n",
    "- Speeds up training\n",
    "- Can improve generalization by focusing on meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer\n",
    "# We use max_features to limit vocabulary size for efficiency\n",
    "# min_df=2 means words must appear in at least 2 documents\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=5000,  # Use top 5000 most frequent words\n",
    "    min_df=2,           # Word must appear in at least 2 documents\n",
    "    max_df=0.95         # Ignore words that appear in >95% of documents\n",
    ")\n",
    "\n",
    "# Transform text to word count matrix\n",
    "# This creates a sparse matrix where each row is a review and each column is a word\n",
    "X = vectorizer.fit_transform(df_normal['text_processed'])\n",
    "y = df_normal['label'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of reviews: {X.shape[0]}\")\n",
    "print(f\"Number of features (words): {X.shape[1]}\")\n",
    "print(f\"Labels: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Feature Matrix:**\n",
    "\n",
    "The `X` matrix is sparse (mostly zeros) because:\n",
    "- Each review contains only a small subset of all possible words\n",
    "- Most words don't appear in most reviews\n",
    "- This is normal and expected for text data\n",
    "\n",
    "Let's visualize what the feature matrix looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense for visualization (only for small samples!)\n",
    "# Note: For large datasets, keep it sparse to save memory\n",
    "X_sample = X[:5].toarray()\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df_features = pd.DataFrame(\n",
    "    X_sample,\n",
    "    columns=feature_names,\n",
    "    index=[f\"Review {i+1}\" for i in range(5)]\n",
    ")\n",
    "\n",
    "# Show only columns (words) that appear in these 5 reviews\n",
    "non_zero_cols = df_features.columns[df_features.sum() > 0]\n",
    "print(f\"Showing {len(non_zero_cols)} words that appear in the first 5 reviews:\")\n",
    "display(df_features[non_zero_cols[:20]])  # Show first 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split Data into Training and Testing Sets\n",
    "\n",
    "**Why split the data?**\n",
    "\n",
    "We need to:\n",
    "1. **Train** the model on one portion of data\n",
    "2. **Test** the model on unseen data to evaluate its performance\n",
    "3. **Prevent overfitting** - ensure the model generalizes to new data\n",
    "\n",
    "**Important:** We split AFTER preprocessing to avoid data leakage (information from test set influencing training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "# stratify=y ensures same class distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} reviews\")\n",
    "print(f\"Test set: {X_test.shape[0]} reviews\")\n",
    "print(\"\\nTraining label distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a Classifier\n",
    "\n",
    "We'll use a **Logistic Regression** classifier, which is:\n",
    "- **Fast** and efficient for text classification\n",
    "- **Well-suited** for count-based features (like our word counts)\n",
    "- **Simple** to understand and interpret\n",
    "- **Effective** for text classification tasks\n",
    "- **Provides probability estimates** for each class\n",
    "\n",
    "**How Logistic Regression works (simplified):**\n",
    "1. Learns weights for each feature (word) that indicate its importance for each class\n",
    "2. Uses a logistic function to convert weighted sums into probabilities\n",
    "3. Predicts the class with highest probability\n",
    "\n",
    "Logistic Regression is a linear classifier that works well with sparse, high-dimensional text features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create and train the classifier\n",
    "# max_iter=1000 ensures convergence for large datasets\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classifier trained successfully!\")\n",
    "print(f\"Number of features learned: {classifier.coef_.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Classifier\n",
    "\n",
    "Now let's evaluate how well our classifier performs on the test set. We'll use several metrics:\n",
    "\n",
    "- **Accuracy**: Overall percentage of correct predictions\n",
    "- **Precision**: Of all predictions for a class, how many were correct?\n",
    "- **Recall**: Of all actual instances of a class, how many did we find?\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balances both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Classification Report:**\n",
    "\n",
    "- **Precision**: When the model predicts a class, how often is it correct?\n",
    "  - High precision = few false positives\n",
    "  \n",
    "- **Recall**: How many of the actual instances of a class did we catch?\n",
    "  - High recall = few false negatives\n",
    "  \n",
    "- **F1-Score**: Balances precision and recall\n",
    "  - Useful when you need a single metric\n",
    "  \n",
    "- **Support**: Number of actual instances of each class in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display as DataFrame for better readability\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"Actual {label}\" for label in classifier.classes_],\n",
    "    columns=[f\"Predicted {label}\" for label in classifier.classes_]\n",
    ")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"(Rows = Actual, Columns = Predicted)\")\n",
    "display(cm_df)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classifier.classes_))\n",
    "plt.xticks(tick_marks, classifier.classes_, rotation=45)\n",
    "plt.yticks(tick_marks, classifier.classes_)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Confusion Matrix:**\n",
    "\n",
    "- **Diagonal elements** (top-left to bottom-right): Correct predictions\n",
    "- **Off-diagonal elements**: Misclassifications\n",
    "  - Example: If \"Actual Negative\" row has a number in \"Predicted Positive\" column, those are negative reviews incorrectly classified as positive\n",
    "\n",
    "The confusion matrix helps us understand:\n",
    "- Which classes are confused with each other\n",
    "- Whether errors are balanced or biased toward certain classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze Important Features\n",
    "\n",
    "Let's see which words are most important for each class. This helps us understand what the model learned and provides interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get coefficients for each class\n",
    "# Higher values = more indicative of that class\n",
    "coefficients = classifier.coef_\n",
    "\n",
    "# Create DataFrame showing top words for each class\n",
    "top_words_per_class = {}\n",
    "for idx, class_label in enumerate(classifier.classes_):\n",
    "    # Get indices of top words for this class\n",
    "    top_indices = np.argsort(coefficients[idx])[-20:][::-1]  # Top 20 words\n",
    "    top_words = [(feature_names[i], coefficients[idx][i]) for i in top_indices]\n",
    "    top_words_per_class[class_label] = top_words\n",
    "\n",
    "# Display results\n",
    "for class_label, words in top_words_per_class.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Top 20 words for class: {class_label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    df_top = pd.DataFrame(words, columns=['Word', 'Coefficient']).style.background_gradient(cmap='Greens')\n",
    "    display(df_top.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting Feature Importance:**\n",
    "\n",
    "The coefficients tell us how strongly each word is associated with each class:\n",
    "- **Higher coefficient** = word is more indicative of that class\n",
    "- **Lower (more negative) coefficient** = word is less indicative of that class\n",
    "- Words with high coefficients for \"Positive\" are likely positive sentiment words\n",
    "- Words with high coefficients for \"Negative\" are likely negative sentiment words\n",
    "\n",
    "**Compare with EDA results:** Do these top words match the words we found in our earlier EDA analysis? This validates that the model learned meaningful patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Test on Sample Reviews\n",
    "\n",
    "Let's test our classifier on some example reviews to see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get test indices in the original dataframe\n",
    "# train_test_split maintains order, so we need to track which rows went to test set\n",
    "_, test_indices_original = train_test_split(\n",
    "    df_normal.index,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df_normal['label']\n",
    ")\n",
    "\n",
    "# Select a few random test examples\n",
    "np.random.seed(SEED)\n",
    "# FIX: For sparse matrices, use shape[0] instead of len()\n",
    "sample_test_indices = np.random.choice(X_test.shape[0], size=5, replace=False)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For a more visual display, let's build a table of predictions and use color to show confidence\n",
    "\n",
    "viz_rows = []\n",
    "for test_idx in sample_test_indices:\n",
    "    original_df_idx = test_indices_original[test_idx]\n",
    "    text_snippet = df_normal.loc[original_df_idx, 'text'][:80] + (\"...\" if len(df_normal.loc[original_df_idx, 'text']) > 80 else \"\")\n",
    "    actual = y_test[test_idx]\n",
    "    prediction = classifier.predict(X_test[test_idx:test_idx+1])[0]\n",
    "    probs = classifier.predict_proba(X_test[test_idx:test_idx+1])[0]\n",
    "    confidence = probs[classifier.classes_.tolist().index(prediction)]\n",
    "    viz_rows.append({\n",
    "        \"Review #\": test_idx,\n",
    "        \"Snippet\": text_snippet,\n",
    "        \"Actual Label\": actual,\n",
    "        \"Predicted Label\": prediction,\n",
    "        \"Confidence\": confidence,\n",
    "        **{f\"P({cls})\": p for cls, p in zip(classifier.classes_, probs)}\n",
    "    })\n",
    "\n",
    "viz_df = pd.DataFrame(viz_rows)\n",
    "# True-match in green, wrong in red\n",
    "def highlight_prediction(row):\n",
    "    color = \"\"\n",
    "    if row['Actual Label'] == row['Predicted Label']:\n",
    "        color = \"background-color: #d4f4dd\"  # light green\n",
    "    else:\n",
    "        color = \"background-color: #f4cccc\"  # light red\n",
    "    return [color]*len(row)\n",
    "# Display styled table, coloring accuracy and using a confidence gradient on \"Confidence\"\n",
    "viz_df_styled = (viz_df.style\n",
    "    .apply(highlight_prediction, axis=1)\n",
    "    .background_gradient(subset=['Confidence'], cmap='Blues')\n",
    "    .format({col: \"{:.2%}\" for col in ['Confidence'] + [f\"P({cls})\" for cls in classifier.classes_]})\n",
    ")\n",
    "display(viz_df_styled)\n",
    "\n",
    "# Also, show barplots of the predicted probability for each review\n",
    "for idx, row in viz_df.iterrows():\n",
    "    plt.figure(figsize=(4,2))\n",
    "    plt.bar(classifier.classes_, [row[f\"P({cls})\"] for cls in classifier.classes_], color=['#d1e0e0' if row['Predicted Label']==cls else '#ffe0b2' for cls in classifier.classes_])\n",
    "    plt.title(f\"Review #{row['Review #']} prediction\\nActual: {row['Actual Label']} | Predicted: {row['Predicted Label']} ({row['Confidence']:.0%})\")\n",
    "    plt.ylabel('Probability')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: What We Learned\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Simple approaches work**: Word counts (Bag of Words) can be effective for text classification\n",
    "- **Preprocessing matters**: The cleaning, normalization, and stemming we did earlier improved our features\n",
    "- **Interpretability**: We can see which words drive predictions, making the model explainable\n",
    "- **Evaluation is crucial**: Always test on unseen data to measure real-world performance\n",
    "\n",
    "**Next Steps (for future exploration):**\n",
    "\n",
    "- Try **TF-IDF** instead of raw counts (weights words by importance)\n",
    "- Experiment with different classifiers (SVM, Random Forest, etc.)\n",
    "- Use **word embeddings** (like Word2Vec or pre-trained embeddings) for richer features\n",
    "- Fine-tune **transformer models** (like BERT) for state-of-the-art performance\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: This is a foundational approach. Modern NLP often uses more sophisticated methods, but understanding word counts and simple classifiers is essential for building intuition about how text classification works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
